{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNG0OWYV71hxFq5AON4P/5F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/210124cse-jpg/CNN_implement/blob/main/210124_CNN_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmuupI42DuqP"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# STEP 1: Clone repo (Colab)\n",
        "# -----------------------------\n",
        "!rm -rf CNN_implement\n",
        "!git clone https://github.com/210124cse-jpg/CNN_implement.git\n",
        "!ls -la CNN_implement\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 2: Install & Imports\n",
        "# -----------------------------\n",
        "!pip -q install scikit-learn\n",
        "\n",
        "import os, glob, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "metadata": {
        "id": "a2zWcgJHEZPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 3: Seed + Device\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "id": "-alzJ_NdEdTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 4: Paths\n",
        "# -----------------------------\n",
        "REPO_DIR = \"/content/CNN_implement\"\n",
        "TRAIN_DIR = os.path.join(REPO_DIR, \"dataset-4\", \"train\")\n",
        "TEST_DIR  = os.path.join(REPO_DIR, \"dataset-4\", \"test\")\n",
        "CUSTOM_DIR = os.path.join(REPO_DIR, \"custom_data\")\n",
        "\n",
        "print(\"TRAIN_DIR :\", TRAIN_DIR)\n",
        "print(\"TEST_DIR  :\", TEST_DIR)\n",
        "print(\"CUSTOM_DIR:\", CUSTOM_DIR)\n",
        "print(\"Custom images:\", len(glob.glob(os.path.join(CUSTOM_DIR, \"*.*\"))))\n"
      ],
      "metadata": {
        "id": "EhoNs5TaEgfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 5: Build consistent class mapping (case-insensitive)\n",
        "# -----------------------------\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n",
        "\n",
        "def list_class_folders(root):\n",
        "    classes = []\n",
        "    for name in os.listdir(root):\n",
        "        full = os.path.join(root, name)\n",
        "        if os.path.isdir(full):\n",
        "            classes.append(name)\n",
        "    return classes\n",
        "\n",
        "train_class_folders = list_class_folders(TRAIN_DIR)\n",
        "canonical_classes = sorted(list({c.lower() for c in train_class_folders}))\n",
        "class_to_idx = {c: i for i, c in enumerate(canonical_classes)}\n",
        "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "\n",
        "print(\"Classes:\", canonical_classes)\n",
        "print(\"Num classes:\", len(canonical_classes))\n"
      ],
      "metadata": {
        "id": "OWQsDV1SEjRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 6: Collect samples from folders (train/test)\n",
        "# -----------------------------\n",
        "def collect_samples(root, class_to_idx):\n",
        "    samples = []\n",
        "    for folder in os.listdir(root):\n",
        "        folder_path = os.path.join(root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        cls = folder.lower()\n",
        "        if cls not in class_to_idx:\n",
        "            continue\n",
        "\n",
        "        label = class_to_idx[cls]\n",
        "        for fname in os.listdir(folder_path):\n",
        "            if fname.lower().endswith(IMG_EXTS):\n",
        "                samples.append((os.path.join(folder_path, fname), label))\n",
        "    return samples\n",
        "\n",
        "train_samples_all = collect_samples(TRAIN_DIR, class_to_idx)\n",
        "test_samples = collect_samples(TEST_DIR, class_to_idx)\n",
        "\n",
        "print(\"Train samples:\", len(train_samples_all))\n",
        "print(\"Test samples :\", len(test_samples))\n",
        "\n",
        "if len(train_samples_all) == 0 or len(test_samples) == 0:\n",
        "    raise ValueError(\"No images found in train/test. Check your dataset-4 folder structure.\")\n"
      ],
      "metadata": {
        "id": "84RswvYnElg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 7: Transforms\n",
        "# -----------------------------\n",
        "IMG_SIZE = 160\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.12),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ],
      "metadata": {
        "id": "GmWITHTzEoF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 8: Dataset class (from samples list)\n",
        "# -----------------------------\n",
        "class SamplesDataset(Dataset):\n",
        "    def __init__(self, samples, transform=None):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label, path\n"
      ],
      "metadata": {
        "id": "CX--cO7VEqkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 9: Train/Val split (80/20) with fixed seed\n",
        "# -----------------------------\n",
        "val_ratio = 0.2\n",
        "n = len(train_samples_all)\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "indices = np.arange(n)\n",
        "rng.shuffle(indices)\n",
        "\n",
        "val_size = int(n * val_ratio)\n",
        "val_idx = indices[:val_size]\n",
        "train_idx = indices[val_size:]\n",
        "\n",
        "train_samples = [train_samples_all[i] for i in train_idx]\n",
        "val_samples   = [train_samples_all[i] for i in val_idx]\n",
        "\n",
        "train_set = SamplesDataset(train_samples, transform=train_tfms)\n",
        "val_set   = SamplesDataset(val_samples,   transform=eval_tfms)\n",
        "test_set  = SamplesDataset(test_samples,  transform=eval_tfms)\n",
        "\n",
        "print(\"Train:\", len(train_set), \"Val:\", len(val_set), \"Test:\", len(test_set))\n"
      ],
      "metadata": {
        "id": "PjbV0m4-EtKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 10: DataLoaders\n",
        "# -----------------------------\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "6WUdSIg7EwUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 11: CNN model\n",
        "# -----------------------------\n",
        "NUM_CLASSES = len(canonical_classes)\n",
        "\n",
        "class BetterCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 160 -> 80\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 80 -> 40\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 40 -> 20\n",
        "\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = BetterCNN(NUM_CLASSES).to(device)\n",
        "model\n"
      ],
      "metadata": {
        "id": "1led6FeEEzRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# STEP 12: Training setup\n",
        "# =========================\n",
        "\n",
        "EPOCHS = 20\n",
        "LR = 1e-3\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.02)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"max\",\n",
        "    factor=0.5,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "print(\"Step 12 setup done ✅\")\n",
        "print(\"Initial LR:\", optimizer.param_groups[0][\"lr\"])\n"
      ],
      "metadata": {
        "id": "niPLLRiHE28J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 13: Train + Validate (save best model)\n",
        "# -----------------------------\n",
        "def run_epoch(model, loader, train_mode=True):\n",
        "    model.train() if train_mode else model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels, _ in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train_mode):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if train_mode:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, 100.0 * correct / total\n",
        "\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    tr_loss, tr_acc = run_epoch(model, train_loader, train_mode=True)\n",
        "    va_loss, va_acc = run_epoch(model, val_loader,   train_mode=False)\n",
        "\n",
        "    train_losses.append(tr_loss); train_accs.append(tr_acc)\n",
        "    val_losses.append(va_loss);   val_accs.append(va_acc)\n",
        "\n",
        "    scheduler.step(va_acc)\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
        "          f\"Train Loss {tr_loss:.4f} Acc {tr_acc:.2f}% | \"\n",
        "          f\"Val Loss {va_loss:.4f} Acc {va_acc:.2f}%\")\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "model.to(device)\n",
        "print(\"Loaded best model ✅ Val Acc:\", best_val_acc)\n"
      ],
      "metadata": {
        "id": "O9Qxb0NAFPh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 14: Plot Loss vs Epochs & Accuracy vs Epochs (Train + Val)\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(train_accs, label=\"Train Accuracy\")\n",
        "plt.plot(val_accs, label=\"Val Accuracy\")\n",
        "plt.title(\"Accuracy vs Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qi7iV5FXF6Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 15: Test Accuracy + Confusion Matrix (TEST set)\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, _ in test_loader:\n",
        "        outputs = model(images.to(device))\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "test_acc = 100.0 * (all_preds == all_labels).mean()\n",
        "print(f\"TEST Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=canonical_classes)\n",
        "disp.plot(cmap=\"Purples\",xticks_rotation=45, values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QYS4s0U2F_5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 16: Show 10 Correct + 3 Incorrect predictions (TEST set)\n",
        "# -----------------------------\n",
        "def unnormalize(img_t):\n",
        "    img = img_t.clone()\n",
        "    for c in range(3):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    return torch.clamp(img, 0, 1)\n",
        "\n",
        "def show_tensor(img_tensor):\n",
        "    img_tensor = unnormalize(img_tensor)\n",
        "    img = img_tensor.permute(1,2,0).numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "correct_examples = []\n",
        "incorrect_examples = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels, paths in test_loader:\n",
        "        outputs = model(images.to(device))\n",
        "        probs = torch.softmax(outputs, dim=1).cpu()\n",
        "        preds = probs.argmax(dim=1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            t = labels[i].item()\n",
        "            p = preds[i].item()\n",
        "            conf = probs[i, p].item()\n",
        "\n",
        "            if t == p and len(correct_examples) < 10:\n",
        "                correct_examples.append((images[i].cpu(), t, p, conf, paths[i]))\n",
        "            if t != p and len(incorrect_examples) < 3:\n",
        "                incorrect_examples.append((images[i].cpu(), t, p, conf, paths[i]))\n",
        "\n",
        "            if len(correct_examples) >= 10 and len(incorrect_examples) >= 3:\n",
        "                break\n",
        "        if len(correct_examples) >= 10 and len(incorrect_examples) >= 3:\n",
        "            break\n",
        "\n",
        "print(\"Collected:\", len(correct_examples), \"correct,\", len(incorrect_examples), \"incorrect\")\n",
        "\n",
        "# Plot 10 correct\n",
        "plt.figure(figsize=(15,6))\n",
        "for k, (img, t, p, conf, path) in enumerate(correct_examples):\n",
        "    plt.subplot(2,5,k+1)\n",
        "    show_tensor(img)\n",
        "    plt.title(f\"Correct\\nT={idx_to_class[t]}\\nP={idx_to_class[p]} ({conf*100:.1f}%)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "# Plot 3 incorrect\n",
        "plt.figure(figsize=(12,4))\n",
        "for k, (img, t, p, conf, path) in enumerate(incorrect_examples):\n",
        "    plt.subplot(1,3,k+1)\n",
        "    show_tensor(img)\n",
        "    plt.title(f\"Incorrect\\nT={idx_to_class[t]}\\nP={idx_to_class[p]} ({conf*100:.1f}%)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ynU-oMU8GIuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 17: Predict 10 custom images (custom_data) + confidence\n",
        "# -----------------------------\n",
        "custom_files = sorted([f for f in glob.glob(os.path.join(CUSTOM_DIR, \"*\"))\n",
        "                       if f.lower().endswith(IMG_EXTS)])[:10]\n",
        "\n",
        "print(\"Custom files:\", [os.path.basename(x) for x in custom_files])\n",
        "\n",
        "model.eval()\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "for i, f in enumerate(custom_files):\n",
        "    img = Image.open(f).convert(\"RGB\")\n",
        "    x = eval_tfms(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        prob = torch.softmax(out, dim=1)[0].cpu().numpy()\n",
        "        pred = int(np.argmax(prob))\n",
        "        conf = float(prob[pred])\n",
        "\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{os.path.basename(f)}\\nPred={idx_to_class[pred]}\\nConf={conf*100:.1f}%\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l4pQtZAJGNe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 18: Save model weights as model/210124.pth + reload check\n",
        "# -----------------------------\n",
        "STUDENT_ID = \"210124\"\n",
        "MODEL_DIR = os.path.join(REPO_DIR, \"model\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "save_path = os.path.join(MODEL_DIR, f\"{STUDENT_ID}.pth\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved model to:\", save_path)\n",
        "\n",
        "model2 = BetterCNN(NUM_CLASSES).to(device)\n",
        "model2.load_state_dict(torch.load(save_path, map_location=device))\n",
        "model2.eval()\n",
        "print(\"Reload successful ✅\")\n"
      ],
      "metadata": {
        "id": "-Q0nxVPbGVmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}